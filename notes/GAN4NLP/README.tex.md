# Generative Adversarial Nets 4 NLP
GAN ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏î‡∏µ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏á‡∏≤‡∏ô Computer Vision ‡πÅ‡∏ï‡πà‡∏ó‡∏≥‡πÑ‡∏° GAN ‡∏ñ‡∏∂‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ô NLP ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡∏Ñ‡∏∏‡∏¢‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏° GAN ‡∏ñ‡∏∂‡∏á‡∏¢‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô NLP ‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏î‡∏ö‡πâ‡∏≤‡∏á.    

‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ GAN ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏Ñ‡∏∑‡∏≠ NLP ‡∏Ñ‡∏∑‡∏≠‡∏õ‡∏Å‡∏ï‡∏¥‡πÅ‡∏•‡πâ‡∏ß image ‡∏à‡∏∞‡∏°‡∏µ output ‡πÉ‡∏ô‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ continuous ‡πÅ‡∏ï‡πà text ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô discrete ‡∏ó‡∏≥‡πÉ‡∏´‡πâ ‡∏î‡∏¥‡∏ü‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏≥ backpropagate ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ 

‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏ß‡∏¥‡∏ò‡∏µ‡∏´‡∏•‡∏±‡∏Å‡πÜ ‡∏≠‡∏¢‡∏π‡πà‡∏™‡∏≤‡∏°‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏î‡∏¥‡∏ü‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÑ‡∏î‡πâ 
* ‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Ç‡∏≠‡∏á‡∏û‡∏ß‡∏Å Reinforcement Learning (RL) ‡∏°‡∏≤‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡∏û‡∏ß‡∏Å RL ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏à‡∏≠‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏à‡∏≤‡∏Å RL algorithm ‡∏Å‡∏±‡∏ö policy gradients ‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ
* [Gumble-softmax approximation](https://arxiv.org/abs/1611.01144) ‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Ñ‡πà‡∏≤ continous ‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô discrete distribution ‡πÑ‡∏î‡πâ 
* ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á discrete output ‡πÇ‡∏î‡∏¢‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ä‡πâ continuous output ‡πÅ‡∏ó‡∏ô (‡πÄ‡∏ä‡πà‡∏ô ‡πÉ‡∏ä‡πâ sentence vector ‡πÅ‡∏ó‡∏ô discrete tokens ‡∏´‡∏•‡∏≤‡∏¢‡πÜ‡∏≠‡∏±‡∏ô)

## RL
‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏µ‡πÇ‡∏à‡∏ó‡∏¢‡πå text generation ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô RL ‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡∏°‡∏≠‡∏á‡∏ß‡πà‡∏≤ text generation ‡πÄ‡∏õ‡πá‡∏ô state-action sequence ‡πÄ‡∏ä‡πà‡∏ô state (s) ‡∏Ñ‡∏∑‡∏≠ text ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å generated ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß ‡∏™‡πà‡∏ß‡∏ô action ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏à‡∏ô‡∏à‡∏ö (‡πÄ‡∏ä‡πà‡∏ô‡∏à‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ,‡πÄ‡∏à‡∏≠ end of sentence token) ‡∏Å‡πá‡πÉ‡∏´‡πâ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ß‡πà‡∏≤ text ‡∏ó‡∏µ‡πà generated ‡∏°‡∏≤‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á GAN, discriminator ‡∏à‡∏∞‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ reward ‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ state ‡∏Å‡πá‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á policy function $\boldsymbol{\pi}(\boldsymbol{a} | \boldsymbol{s}, \boldsymbol{\theta})$  ‡πÇ‡∏î‡∏¢ ùúÉ ‡∏Ñ‡∏∑‡∏≠ ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå  ‡∏™‡πà‡∏ß‡∏ô output ‡∏Ñ‡∏∑‡∏≠ distribution ‡∏Ç‡∏≠‡∏á action ‡∏ï‡πà‡∏≤‡∏á‡πÜ (given state). ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞ generate text ‡πÇ‡∏î‡∏¢ sample ‡∏à‡∏≤‡∏Å policy            
           
‡∏ñ‡πâ‡∏≤ **J(ùúÉ)** ‡∏Ñ‡∏∑‡∏≠‡∏°‡∏≤‡∏ï‡∏£‡∏ß‡∏±‡∏î performance ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå **ùúÉ*** ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ performance ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ö‡∏ô **J(ùúÉ)** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏´‡∏≤ ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå **ùúÉ*** ‡∏ó‡∏µ‡πà maximize performance ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ó‡∏≥ gradient ascent: $\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}+\alpha \widehat{\nabla J\left(\boldsymbol{\theta}_{t}\right)}$ ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ô‡∏µ‡πâ‡∏´‡∏°‡∏ß‡∏Å (^) ‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤ gradient ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏°‡∏≤ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ concept ‡∏Ç‡∏≠‡∏á   [policy gradient theorem](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#policy-gradient-theorem): $\nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta})$         
          
* **ùúá(s)** ‡∏ß‡∏±‡∏î‡∏ß‡πà‡∏≤ agent ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÑ‡∏õ‡∏ó‡∏µ‡πà state **s** ‡∏ö‡πà‡∏≠‡∏¢‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏Å‡∏ó‡∏≥‡∏ï‡∏≤‡∏° policy **ùúã**
* **q(s, a)** ‡∏ß‡∏±‡∏î‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á action **a** ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏ô state **s** 

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏î‡πâ‡∏≤‡∏ô‡∏Ç‡∏ß‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ö‡∏ß‡∏Å‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å  action **a** ‡πÅ‡∏•‡∏∞‡∏ó‡∏∏‡∏Å state **s** ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô  a ‡∏Å‡∏±‡∏ö s ‡∏ó‡∏µ‡πà timestep t  (s_t,a_t) ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ REINFORCE algorithm ‡∏°‡∏≤‡∏ä‡πà‡∏ß‡∏¢

‡πÄ‡∏°‡∏∑‡πà‡∏≠  **ùúá(s)** ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà state **s** ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏Å‡∏ó‡∏≥‡∏ï‡∏≤‡∏° policy **ùúã**  ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏ß‡∏Å‡∏£‡∏ß‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏ô‡∏≠‡∏Å(outer summation)‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô[expected value](https://en.wikipedia.org/wiki/Expected_value)‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏ß‡∏Å‡∏£‡∏ß‡∏°‡∏î‡πâ‡∏≤‡∏ô‡πÉ‡∏ô(inner summation):                   
$\mathbb{E}_{\pi}\left[\sum_{a} q_{\pi}\left(S_{t}, a\right) \nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right)\right]$            

Note: ‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡∏Ç‡∏≠‡∏á expected value ‡∏Ç‡∏≠‡∏á X ‡∏Ñ‡∏∑‡∏≠ $\mathrm{E}[X]=\sum_{i=1}^{k} x_{i} p_{i}=x_{1} p_{1}+x_{2} p_{2}+\cdots+x_{k} p_{k}$ ‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà p ‡∏Ñ‡∏∑‡∏≠ prob               

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å state ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (s_t) ‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡∏ó‡∏≥‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å action ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÑ‡∏î‡πâ action (a_t)                    
‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡∏ß‡πà‡∏≤ $\pi\left(a | S_{t}, \boldsymbol{\theta}\right)$ ‡∏Ñ‡∏∑‡∏≠‡∏ü‡∏±‡∏á‡∏ä‡∏±‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏à‡∏Å‡πÅ‡∏à‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô(probability distribution)‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞action ‡∏ó‡∏µ‡πàstate s_t ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏≠‡∏µ‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞summation ‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô‡∏Å‡πá‡∏î‡∏π‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡πÜ expected value ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏π‡∏ì‡πÅ‡∏•‡∏∞‡∏´‡∏≤‡∏£‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢$\pi\left(a | S_{t}, \boldsymbol{\theta}\right)$ ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏∞‡πÑ‡∏î‡πâ:            
$\mathbb{E}_{\pi}\left[q_{\pi}\left(S_{t}, A_{t}\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}\right]$

## Reading List
[GAN - NAACL 2019](https://drive.google.com/drive/folders/1E4uHe4_TD4yDJws3t1kXJQanUFJiqpBB)                  
[Generative Adversarial Networks for Text Generation (blog post)](https://becominghuman.ai/generative-adversarial-networks-for-text-generation-part-1-2b886c8cab10)                 
[SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14344/14489)
